<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Overview on ADUDA</title><link>https://aduda.engine210.site/detail/</link><description>Recent content in Overview on ADUDA</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 28 Nov 2018 15:14:39 +1000</lastBuildDate><atom:link href="https://aduda.engine210.site/detail/index.xml" rel="self" type="application/rss+xml"/><item><title>Related Works</title><link>https://aduda.engine210.site/detail/related_works/</link><pubDate>Mon, 29 Nov 2021 13:33:03 +0800</pubDate><guid>https://aduda.engine210.site/detail/related_works/</guid><description>UDA-SST In real life, it is usually expensive to obtain annotated data, especially for dense prediction tasks like semantic segmentation. Therefore, synthetic images with automatically generated labels are used to train segmentation models. However, the models trained from synthetic data are hard to transfer to real data without labels due to the large domain gap. Unsupervised Domain Adaptation for Semantic Segmentation Task (UDA-SST) is a promising method to solve this problem.</description></item><item><title>Method</title><link>https://aduda.engine210.site/detail/method/</link><pubDate>Mon, 29 Nov 2021 14:03:00 +0800</pubDate><guid>https://aduda.engine210.site/detail/method/</guid><description>Our framework mainly consists of two modules: the segmentation model and the RL agent. The segmentation model produces the semantic segmentation map of the raw input image, and the RL agent will make decisions based on the segmentation map. In the following, we first elaborate on how we trained our segmentation model and RL agent. Then, we show how we deploy our models and perform the autonomous driving task in the real world.</description></item></channel></rss>