<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Overview on ADUDA</title><link>https://25349023.github.io/ADUDA/detail/</link><description>Recent content in Overview on ADUDA</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 29 Nov 2021 14:05:15 +0800</lastBuildDate><atom:link href="https://25349023.github.io/ADUDA/detail/index.xml" rel="self" type="application/rss+xml"/><item><title>Related Works</title><link>https://25349023.github.io/ADUDA/detail/related_works/</link><pubDate>Mon, 29 Nov 2021 13:33:03 +0800</pubDate><guid>https://25349023.github.io/ADUDA/detail/related_works/</guid><description>UDA-SST In real life, it is usually expensive to obtain annotated data, especially for dense prediction tasks like semantic segmentation. Therefore, synthetic images with automatically generated labels are used to train segmentation models. However, the models trained from synthetic data are hard to transfer to real data without labels due to the large domain gap. Unsupervised Domain Adaptation for Semantic Segmentation Task (UDA-SST) is a promising method to solve this problem.</description></item><item><title>Method</title><link>https://25349023.github.io/ADUDA/detail/method/</link><pubDate>Mon, 29 Nov 2021 14:03:00 +0800</pubDate><guid>https://25349023.github.io/ADUDA/detail/method/</guid><description>Our framework mainly consists of two modules: the segmentation model and the RL agent. The segmentation model produces the semantic segmentation map of the raw input image, and the RL agent will make decisions based on the segmentation map. In the following, we first elaborate on how we trained our segmentation model and RL agent. Then, we show how we deploy our models and perform the autonomous driving task in the real world.</description></item><item><title>Results</title><link>https://25349023.github.io/ADUDA/detail/results/</link><pubDate>Mon, 29 Nov 2021 14:03:08 +0800</pubDate><guid>https://25349023.github.io/ADUDA/detail/results/</guid><description>UDA-SST We conduct several experiments on two synthetic-to-real adaptation tasks to demonstrate the effectiveness of our UDA techniques. The two synthetic-to-real adaptation tasks are GTA5 $\to$ Cityscapes and GTA5 $\to$ NTHU. To be specific, we validate our method by comparing with the previous work in the first task GTA5 $\to$ Cityscapes. In the second task, we present the adaptation result on our dataset NTHU.
GTA5 $\to$ Cityscapes GTA5 $\to$ Cityscapes is the standard benchmark for UDA-SST.</description></item><item><title>Conclusion</title><link>https://25349023.github.io/ADUDA/detail/conclusion/</link><pubDate>Mon, 29 Nov 2021 14:05:15 +0800</pubDate><guid>https://25349023.github.io/ADUDA/detail/conclusion/</guid><description>In this paper, we propose four easy-to-implement methods: consistency learning, edge prediction, color quantization, and gray-world algorithm which are compatible with the existing UDA-SST works. The proposed methods combined with the existing works outperform state-of-the-art methods on the GTA5 $\to$ Cityscapes benchmark significantly. Furthermore, we integrate our UDA-SST method with RL agent and deploy it to the edge device Husky A200. Ultimately, we develop a sim-to-real autonomous driving car. With a robust vision model and intelligent agent, our end-to-end self-driving system finally shows great adaptability to the NTHU campus and performs exceptionally in the autonomous driving task.</description></item></channel></rss>