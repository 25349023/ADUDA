<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<title>Overview - ADUDA</title>
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel=icon href=https://aduda.engine210.site/favicon.png>
<link rel=stylesheet href=/css/style.min.6719d72e323121369d5caeb4264b95d8a63cf1deaab029da67602fe5a046e0a4.css>
</head>
<body class="page page-default-list">
<div id=main-menu-mobile class=main-menu-mobile>
<ul>
<li class=menu-item-home>
<a href=/>
<span>Home</span>
</a>
</li>
<li class=menu-item-detail>
<a href=/detail/>
<span>Detail</span>
</a>
</li>
</ul>
</div>
<div class=wrapper>
<div class=header>
<div class=container>
<div class=logo>
<a href=https://aduda.engine210.site/><img alt=Logo src=/husky_logo.png></a>
</div>
<div class=logo-mobile>
<a href=https://aduda.engine210.site/><img alt=Logo src=/husky_logo.png></a>
</div>
<div id=main-menu class=main-menu>
<ul>
<li class=menu-item-home>
<a href=/>
<span>Home</span>
</a>
</li>
<li class=menu-item-detail>
<a href=/detail/>
<span>Detail</span>
</a>
</li>
</ul>
</div>
<button id=toggle-main-menu-mobile class="hamburger hamburger--slider" type=button>
<span class=hamburger-box>
<span class=hamburger-inner></span>
</span>
</button>
</div>
</div>
<div class="container pt-2 pt-md-6 pb-3 pb-md-6">
<div class=row>
<div class="col-12 col-md-3 mb-3">
<div class=sidebar>
<div class=docs-menu>
<h4>Detail</h4>
<ul>
<li>
<a href=https://aduda.engine210.site/detail/related_works/>Related Works</a>
</li>
<li>
<a href=https://aduda.engine210.site/detail/method/>Method</a>
</li>
<li>
<a href=https://aduda.engine210.site/detail/results/>Results</a>
</li>
<li>
<a href=https://aduda.engine210.site/detail/conclusion/>Conclusion</a>
</li>
</ul>
</div>
</div>
</div>
<div class="col-12 col-md-9">
<span class=overview>Overview</span>
<h1 class=title>Overview</h1>
<div class=content>
<h2 id=introduction>Introduction</h2>
<p>Autonomous Driving has become one of the most popular research fields in recent years, enabling a car to sense the environment and drive without human manipulation. However, this task is highly challenging because of the rapidly-varying environment in the real world. Also, training an agent in the real world seems infeasible due to the unaffordable trial-and-error cost. Fortunately, Sim-to-Real methods help us deal with the problem by training the model in simulation environments and adapting it to the real world. It also enhances the robustness of the model because we can create and utilize unlimited amounts of training data in the simulation environment.</p>
<p>Semantic Segmentation is crucial information to self-driving agents. However, training a deep neural network requires a significant amount of labeled data, especially for dense prediction tasks such as Semantic Segmentation. It will be highly time-consuming and expensive if we have to build a dataset for every single task. Thus, we want to leverage the synthetic labeled data generated by simulation environments. Unfortunately, there exists a domain gap between synthetic data and real-world data. UDA-SST is a promising technique to minimize the domain gap, producing a high-quality segmentation map. We train the UDA-SST model with the GTA5 dataset and raw images we collected from the campus to generate segmentation maps without extra labeling effort.</p>
<p>Currently, there exist several well-known UDA-SST works. We propose four effective techniques which can be easily integrated with the current works. First, we adopt consistency learning, helping the model to learn robust and general knowledge. Further, we replace depth information used in CorDA\cite{wang2021domain} with edge information which is easier to acquire. Lastly, we use the quantization and gray world technique to calibrate images. Combining these techniques, we achieve state-of-the-art performance in UDA-SST works.</p>
<p>To build simulation environments for Sim-to-Real methods, we create a virtual world with the 3D game engine Unity. The environment looks like the street view we can see in our daily lives, including roads, sidewalks, pedestrians, trees, bicycles, cars, and terrain. We train an RL agent in the Unity environment, where it can only drive on the road and dodge all the obstacles. Finally, we combine RL and UDA-SST to achieve obstacle avoidance on roads of NTHU. The RL agent will make decisions based on the segmentation maps predicted by the UDA-SST model. With the Sim-to-Real approach, our agent demonstrates exceptional performance in both the simulation environment and the real world.</p>
<p>In a nutshell, our contributions are as follows:</p>
<ul>
<li>We propose four easy-to-implement techniques that are compatible with the existing UDA-SST works and can bring considerable improvement.</li>
<li>By combining our proposed techniques with the existing self-training framework, we achieve state-of-the-art performance on the UDA-SST benchmark.</li>
<li>We develop a sim-to-real autonomous driving car by combining UDA-SST and RL methods and prove that our UDA-SST techniques are efficient and practical.</li>
</ul>
</div>
<div class="summary mb-2">
<h2 class=title-summary><a href=https://aduda.engine210.site/detail/related_works/>Related Works</a></h2>
<p>UDA-SST In real life, it is usually expensive to obtain annotated data, especially for dense prediction tasks like semantic segmentation. Therefore, synthetic images with automatically generated labels are used to train segmentation models. However, the models trained from synthetic data are hard to transfer to real data without labels due to the large domain gap. Unsupervised Domain Adaptation for Semantic Segmentation Task (UDA-SST) is a promising method to solve this problem.</p>
</div>
<div class="summary mb-2">
<h2 class=title-summary><a href=https://aduda.engine210.site/detail/method/>Method</a></h2>
<p>Our framework mainly consists of two modules: the segmentation model and the RL agent. The segmentation model produces the semantic segmentation map of the raw input image, and the RL agent will make decisions based on the segmentation map. In the following, we first elaborate on how we trained our segmentation model and RL agent. Then, we show how we deploy our models and perform the autonomous driving task in the real world.</p>
</div>
<div class="summary mb-2">
<h2 class=title-summary><a href=https://aduda.engine210.site/detail/results/>Results</a></h2>
<p>UDA-SST We conduct several experiments on two synthetic-to-real adaptation tasks to demonstrate the effectiveness of our UDA techniques. The two synthetic-to-real adaptation tasks are GTA5 $\to$ Cityscapes and GTA5 $\to$ NTHU. To be specific, we validate our method by comparing with the previous work in the first task GTA5 $\to$ Cityscapes. In the second task, we present the adaptation result on our dataset NTHU.
GTA5 $\to$ Cityscapes GTA5 $\to$ Cityscapes is the standard benchmark for UDA-SST.</p>
</div>
<div class="summary mb-2">
<h2 class=title-summary><a href=https://aduda.engine210.site/detail/conclusion/>Conclusion</a></h2>
<p>In this paper, we propose four easy-to-implement methods: consistency learning, edge prediction, color quantization, and gray-world algorithm which are compatible with the existing UDA-SST works. The proposed methods combined with the existing works outperform state-of-the-art methods on the GTA5 $\to$ Cityscapes benchmark significantly. Furthermore, we integrate our UDA-SST method with RL agent and deploy it to the edge device Husky A200. Ultimately, we develop a sim-to-real autonomous driving car. With a robust vision model and intelligent agent, our end-to-end self-driving system finally shows great adaptability to the NTHU campus and performs exceptionally in the autonomous driving task.</p>
</div>
</div>
</div>
</div>
</div>
<div class=sub-footer>
<div class=container>
<div class=row>
<div class=col-12>
<div class=sub-footer-inner>
<ul>
</ul>
</div>
</div>
</div>
</div>
</div>
<script type=text/javascript src=/js/scripts.min.eaf147370baecdd07c022597db631f99cab1c9cd6479de586f30327a568d6a0f.js></script>
</body>
</html>